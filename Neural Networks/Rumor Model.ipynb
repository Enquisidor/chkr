{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "emergent.csv                                   2019-09-20 11:24:30      1681978\n",
      "politifact.csv                                 2019-09-20 11:24:30      1745905\n",
      "snopes.csv                                     2019-09-20 11:24:30      6191626\n",
      "\n",
      "\n",
      "\n",
      "Rumor Citation Emergent:\n",
      "Index(['emergent_page', 'claim', 'claim_description', 'claim_label', 'tags',\n",
      "       'claim_source_domain', 'claim_course_url', 'date', 'body',\n",
      "       'page_domain', 'page_url', 'page_headline', 'page_position',\n",
      "       'page_shares', 'page_order'],\n",
      "      dtype='object')\n",
      "\n",
      "Rumor Citation Politifact:\n",
      "Index(['politifact_page', 'claim', 'claim_source', 'claim_citation',\n",
      "       'claim_label', 'date_published', 'researched_by', 'edited_by', 'tags',\n",
      "       'page_citation', 'page_url', 'page_is_first_citation'],\n",
      "      dtype='object')\n",
      "\n",
      "Rumor Citation Snopes:\n",
      "Index(['snopes_page', 'topic', 'claim', 'claim_label', 'date_published',\n",
      "       'date_updated', 'page_url', 'page_is_example', 'page_is_image_credit',\n",
      "       'page_is_archived', 'page_is_first_citation', 'tags'],\n",
      "      dtype='object')\n",
      "\n",
      "Rumor Citation Emergent:\n",
      "(2145, 15)\n",
      "\n",
      "Rumor Citation Politifact:\n",
      "(2923, 12)\n",
      "\n",
      "Rumor Citation Snopes:\n",
      "(16865, 12)\n",
      "\n",
      "Rumor Citation Emergent:\n",
      "emergent_page          0\n",
      "claim                  0\n",
      "claim_description      0\n",
      "claim_label            0\n",
      "tags                   0\n",
      "claim_source_domain    0\n",
      "claim_course_url       0\n",
      "date                   0\n",
      "body                   0\n",
      "page_domain            0\n",
      "page_url               0\n",
      "page_headline          0\n",
      "page_shares            0\n",
      "page_order             0\n",
      "dtype: int64\n",
      "\n",
      "Rumor Citation Politifact:\n",
      "politifact_page           0\n",
      "claim                     0\n",
      "claim_source              0\n",
      "claim_citation            0\n",
      "claim_label               0\n",
      "date_published            0\n",
      "researched_by             0\n",
      "edited_by                 0\n",
      "tags                      0\n",
      "page_citation             0\n",
      "page_url                  0\n",
      "page_is_first_citation    0\n",
      "dtype: int64\n",
      "\n",
      "Rumor Citation Snopes:\n",
      "snopes_page               0\n",
      "topic                     0\n",
      "claim                     0\n",
      "claim_label               0\n",
      "date_published            0\n",
      "date_updated              0\n",
      "page_url                  0\n",
      "page_is_example           0\n",
      "page_is_image_credit      0\n",
      "page_is_archived          0\n",
      "page_is_first_citation    0\n",
      "tags                      0\n",
      "dtype: int64\n",
      "0       Daiane+DeJesus,DC+Toys+Collector,Porm,Sandy+Su...\n",
      "1       Daiane+DeJesus,DC+Toys+Collector,Porm,Sandy+Su...\n",
      "2       Daiane+DeJesus,DC+Toys+Collector,Porm,Sandy+Su...\n",
      "3       Australia,Food,Hamburger,McDonald's,Quarter+Po...\n",
      "4       Australia,Food,Hamburger,McDonald's,Quarter+Po...\n",
      "                              ...                        \n",
      "2140    Apple+Watch,Daisuke+Wakabayashi,Wall+Street+Jo...\n",
      "2141    Apple+Watch,Daisuke+Wakabayashi,Wall+Street+Jo...\n",
      "2142    Apple+Watch,Daisuke+Wakabayashi,Wall+Street+Jo...\n",
      "2143    Apple+Watch,Daisuke+Wakabayashi,Wall+Street+Jo...\n",
      "2144    Apple+Watch,Daisuke+Wakabayashi,Wall+Street+Jo...\n",
      "Name: tags, Length: 2144, dtype: object\n",
      "0       Fake news\n",
      "1       Fake news\n",
      "2       Fake news\n",
      "3       Fake news\n",
      "4       Fake news\n",
      "          ...    \n",
      "2918      History\n",
      "2919      History\n",
      "2920      History\n",
      "2921      History\n",
      "2922      History\n",
      "Name: tags, Length: 2923, dtype: object\n",
      "0                                              \n",
      "1                                              \n",
      "2                                              \n",
      "3                                              \n",
      "4                                              \n",
      "                          ...                  \n",
      "16860    politics,guatemala,donald-trump,mexico\n",
      "16861    politics,guatemala,donald-trump,mexico\n",
      "16862    politics,guatemala,donald-trump,mexico\n",
      "16863    politics,guatemala,donald-trump,mexico\n",
      "16864    politics,guatemala,donald-trump,mexico\n",
      "Name: tags, Length: 16862, dtype: object\n",
      "Unverified    857\n",
      "TRUE          737\n",
      "FALSE         550\n",
      "Name: claim_label, dtype: int64\n",
      "pants-fire     1110\n",
      "false           731\n",
      "barely-true     460\n",
      "half-true       244\n",
      "mostly-true     207\n",
      "true            171\n",
      "Name: claim_label, dtype: int64\n",
      "false           8765\n",
      "mfalse          3319\n",
      "mixture         2618\n",
      "true            1480\n",
      "mtrue            487\n",
      "undetermined     151\n",
      "legend            42\n",
      "Name: claim_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "%run 'Load & Clean Rumor'.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumor_dfs_cleaned = rumor_dfs\n",
    "\n",
    "rumor_dfs_cleaned[0] = rumor_dfs_cleaned[0].drop('emergent_page', axis=1)\n",
    "rumor_dfs_cleaned[0] = rumor_dfs_cleaned[0].drop('body', axis=1)\n",
    "rumor_dfs_cleaned[0] = rumor_dfs_cleaned[0].drop('claim', axis=1)\n",
    "rumor_dfs_cleaned[0] = rumor_dfs_cleaned[0].drop('claim_description', axis=1)\n",
    "rumor_dfs_cleaned[0] = rumor_dfs_cleaned[0].drop('claim_course_url', axis=1)\n",
    "rumor_dfs_cleaned[0] = rumor_dfs_cleaned[0].drop('date', axis=1)\n",
    "rumor_dfs_cleaned[0] = rumor_dfs_cleaned[0].drop('page_url', axis=1)\n",
    "rumor_dfs_cleaned[0] = rumor_dfs_cleaned[0].drop('page_headline', axis=1)\n",
    "rumor_dfs_cleaned[0] = rumor_dfs_cleaned[0].drop('page_order', axis=1)\n",
    "rumor_dfs_cleaned[0].claim_label = rumor_dfs_cleaned[0].claim_label.astype('category').cat.codes\n",
    "\n",
    "rumor_dfs_cleaned[1] = rumor_dfs_cleaned[1].drop('politifact_page', axis=1)\n",
    "rumor_dfs_cleaned[1] = rumor_dfs_cleaned[1].drop('claim', axis=1)\n",
    "rumor_dfs_cleaned[1] = rumor_dfs_cleaned[1].drop('claim_citation', axis=1)\n",
    "rumor_dfs_cleaned[1] = rumor_dfs_cleaned[1].drop('date_published', axis=1)\n",
    "rumor_dfs_cleaned[1] = rumor_dfs_cleaned[1].drop('page_citation', axis=1)\n",
    "rumor_dfs_cleaned[1] = rumor_dfs_cleaned[1].drop('page_url', axis=1)\n",
    "rumor_dfs_cleaned[1] = rumor_dfs_cleaned[1].drop('page_is_first_citation', axis=1)\n",
    "rumor_dfs_cleaned[1].claim_label = rumor_dfs_cleaned[1].claim_label.astype('category').cat.codes\n",
    "\n",
    "rumor_dfs_cleaned[2] = rumor_dfs_cleaned[2].drop('snopes_page', axis=1)\n",
    "rumor_dfs_cleaned[2] = rumor_dfs_cleaned[2].drop('claim', axis=1)\n",
    "rumor_dfs_cleaned[2] = rumor_dfs_cleaned[2].drop('date_published', axis=1)\n",
    "rumor_dfs_cleaned[2] = rumor_dfs_cleaned[2].drop('date_updated', axis=1)\n",
    "rumor_dfs_cleaned[2] = rumor_dfs_cleaned[2].drop('page_url', axis=1)\n",
    "rumor_dfs_cleaned[2] = rumor_dfs_cleaned[2].drop('page_is_first_citation', axis=1)\n",
    "rumor_dfs_cleaned[2].claim_label = rumor_dfs_cleaned[2].claim_label.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2144, 5) (2923, 5) (16862, 6)\n",
      "2    857\n",
      "1    737\n",
      "0    550\n",
      "Name: claim_label, dtype: int64 4    1110\n",
      "1     731\n",
      "0     460\n",
      "2     244\n",
      "3     207\n",
      "5     171\n",
      "Name: claim_label, dtype: int64 0    8765\n",
      "2    3319\n",
      "3    2618\n",
      "5    1480\n",
      "4     487\n",
      "6     151\n",
      "1      42\n",
      "Name: claim_label, dtype: int64\n",
      "Index(['claim_label', 'tags', 'claim_source_domain', 'page_domain',\n",
      "       'page_shares'],\n",
      "      dtype='object') Index(['claim_source', 'claim_label', 'researched_by', 'edited_by', 'tags'], dtype='object') Index(['topic', 'claim_label', 'page_is_example', 'page_is_image_credit',\n",
      "       'page_is_archived', 'tags'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(rumor_dfs_cleaned[0].shape, rumor_dfs_cleaned[1].shape, rumor_dfs_cleaned[2].shape)\n",
    "print(rumor_dfs_cleaned[0].claim_label.value_counts(), rumor_dfs_cleaned[1].claim_label.value_counts(), rumor_dfs_cleaned[2].claim_label.value_counts())\n",
    "print(rumor_dfs_cleaned[0].columns, rumor_dfs_cleaned[1].columns, rumor_dfs_cleaned[2].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [] \n",
    "X_test = [] \n",
    "y_train = [] \n",
    "y_test = []\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "for i in range(0,2):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "         rumor_one_hots[i].drop('claim_label', axis=1), rumor_one_hots[i].claim_label,\n",
    "        test_size=0.33, random_state=42)\n",
    "\n",
    "    X_train.append(X_tr)\n",
    "    X_test.append(X_te)\n",
    "    y_train.append(y_tr)\n",
    "    y_test.append(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
