{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "emergent.csv                                   2019-09-20 11:24:30      1681978\n",
      "politifact.csv                                 2019-09-20 11:24:30      1745905\n",
      "snopes.csv                                     2019-09-20 11:24:30      6191626\n",
      "\n",
      "\n",
      "\n",
      "Rumor Citation Emergent:\n",
      "Index(['emergent_page', 'claim', 'claim_description', 'claim_label', 'tags',\n",
      "       'claim_source_domain', 'claim_course_url', 'date', 'body',\n",
      "       'page_domain', 'page_url', 'page_headline', 'page_position',\n",
      "       'page_shares', 'page_order'],\n",
      "      dtype='object')\n",
      "\n",
      "Rumor Citation Politifact:\n",
      "Index(['politifact_page', 'claim', 'claim_source', 'claim_citation',\n",
      "       'claim_label', 'date_published', 'researched_by', 'edited_by', 'tags',\n",
      "       'page_citation', 'page_url', 'page_is_first_citation'],\n",
      "      dtype='object')\n",
      "\n",
      "Rumor Citation Snopes:\n",
      "Index(['snopes_page', 'topic', 'claim', 'claim_label', 'date_published',\n",
      "       'date_updated', 'page_url', 'page_is_example', 'page_is_image_credit',\n",
      "       'page_is_archived', 'page_is_first_citation', 'tags'],\n",
      "      dtype='object')\n",
      "\n",
      "Rumor Citation Emergent:\n",
      "(2145, 15)\n",
      "\n",
      "Rumor Citation Politifact:\n",
      "(2923, 12)\n",
      "\n",
      "Rumor Citation Snopes:\n",
      "(16865, 12)\n",
      "\n",
      "Rumor Citation Emergent:\n",
      "emergent_page          0\n",
      "claim                  0\n",
      "claim_description      0\n",
      "claim_label            0\n",
      "tags                   0\n",
      "claim_source_domain    0\n",
      "claim_course_url       0\n",
      "date                   0\n",
      "body                   0\n",
      "page_domain            0\n",
      "page_url               0\n",
      "page_headline          0\n",
      "page_shares            0\n",
      "page_order             0\n",
      "dtype: int64\n",
      "\n",
      "Rumor Citation Politifact:\n",
      "politifact_page           0\n",
      "claim                     0\n",
      "claim_source              0\n",
      "claim_citation            0\n",
      "claim_label               0\n",
      "date_published            0\n",
      "researched_by             0\n",
      "edited_by                 0\n",
      "tags                      0\n",
      "page_citation             0\n",
      "page_url                  0\n",
      "page_is_first_citation    0\n",
      "dtype: int64\n",
      "\n",
      "Rumor Citation Snopes:\n",
      "snopes_page               0\n",
      "topic                     0\n",
      "claim                     0\n",
      "claim_label               0\n",
      "date_published            0\n",
      "date_updated              0\n",
      "page_url                  0\n",
      "page_is_example           0\n",
      "page_is_image_credit      0\n",
      "page_is_archived          0\n",
      "page_is_first_citation    0\n",
      "tags                      0\n",
      "dtype: int64\n",
      "0       Daiane+DeJesus,DC+Toys+Collector,Porm,Sandy+Su...\n",
      "1       Daiane+DeJesus,DC+Toys+Collector,Porm,Sandy+Su...\n",
      "2       Daiane+DeJesus,DC+Toys+Collector,Porm,Sandy+Su...\n",
      "3       Australia,Food,Hamburger,McDonald's,Quarter+Po...\n",
      "4       Australia,Food,Hamburger,McDonald's,Quarter+Po...\n",
      "                              ...                        \n",
      "2140    Apple+Watch,Daisuke+Wakabayashi,Wall+Street+Jo...\n",
      "2141    Apple+Watch,Daisuke+Wakabayashi,Wall+Street+Jo...\n",
      "2142    Apple+Watch,Daisuke+Wakabayashi,Wall+Street+Jo...\n",
      "2143    Apple+Watch,Daisuke+Wakabayashi,Wall+Street+Jo...\n",
      "2144    Apple+Watch,Daisuke+Wakabayashi,Wall+Street+Jo...\n",
      "Name: tags, Length: 2144, dtype: object\n",
      "0       Fake news\n",
      "1       Fake news\n",
      "2       Fake news\n",
      "3       Fake news\n",
      "4       Fake news\n",
      "          ...    \n",
      "2918      History\n",
      "2919      History\n",
      "2920      History\n",
      "2921      History\n",
      "2922      History\n",
      "Name: tags, Length: 2923, dtype: object\n",
      "0                                              \n",
      "1                                              \n",
      "2                                              \n",
      "3                                              \n",
      "4                                              \n",
      "                          ...                  \n",
      "16860    politics,guatemala,donald-trump,mexico\n",
      "16861    politics,guatemala,donald-trump,mexico\n",
      "16862    politics,guatemala,donald-trump,mexico\n",
      "16863    politics,guatemala,donald-trump,mexico\n",
      "16864    politics,guatemala,donald-trump,mexico\n",
      "Name: tags, Length: 16862, dtype: object\n",
      "Unverified    857\n",
      "TRUE          737\n",
      "FALSE         550\n",
      "Name: claim_label, dtype: int64\n",
      "pants-fire     1110\n",
      "false           731\n",
      "barely-true     460\n",
      "half-true       244\n",
      "mostly-true     207\n",
      "true            171\n",
      "Name: claim_label, dtype: int64\n",
      "false           8765\n",
      "mfalse          3319\n",
      "mixture         2618\n",
      "true            1480\n",
      "mtrue            487\n",
      "undetermined     151\n",
      "legend            42\n",
      "Name: claim_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "%run 'Load & Clean Rumor'.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumor_dfs_claim = []\n",
    "rumor_dfs_claim.append(p.DataFrame(rumor_dfs[0].claim))\n",
    "rumor_dfs_claim.append(p.DataFrame(rumor_dfs[1].claim))\n",
    "rumor_dfs_claim.append(p.DataFrame(rumor_dfs[2].claim))\n",
    "rumor_dfs_claim[0]['label'] = rumor_dfs[0].claim_label\n",
    "rumor_dfs_claim[1]['label'] = rumor_dfs[1].claim_label\n",
    "rumor_dfs_claim[2]['label'] = rumor_dfs[2].claim_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unverified    857\n",
      "TRUE          737\n",
      "FALSE         550\n",
      "Name: label, dtype: int64 pants-fire     1110\n",
      "false           731\n",
      "barely-true     460\n",
      "half-true       244\n",
      "mostly-true     207\n",
      "true            171\n",
      "Name: label, dtype: int64 false           8765\n",
      "mfalse          3319\n",
      "mixture         2618\n",
      "true            1480\n",
      "mtrue            487\n",
      "undetermined     151\n",
      "legend            42\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rumor_dfs_claim[0].label.value_counts(), rumor_dfs_claim[1].label.value_counts(), rumor_dfs_claim[2].label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [] \n",
    "X_test = [] \n",
    "y_train = [] \n",
    "y_test = []\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for i in range(0,3):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "         rumor_dfs_claim[i].drop('label', axis=1), rumor_dfs_claim[i].label,\n",
    "        test_size=0.33, random_state=42)\n",
    "\n",
    "    X_train.append(X_tr)\n",
    "    X_test.append(X_te)\n",
    "    y_train.append(y_tr)\n",
    "    y_test.append(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 11\n",
      "\t95percentile : 18\n",
      "\t99percentile : 24\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 11\n",
      "\t95percentile : 18\n",
      "\t99percentile : 21\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ktrain \n",
    "from ktrain import text \n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "t = text.Transformer(MODEL_NAME, maxlen=500)\n",
    "trn = t.preprocess_train(X_train[0].claim.tolist(), y_train[0].values.tolist())\n",
    "val = t.preprocess_test(X_test[0].claim.tolist(), y_test[0].values.tolist())\n",
    "model = t.get_classifier()\n",
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating training for different learning rates... this may take a few moments...\n",
      "Train for 239 steps\n",
      "Epoch 1/2\n",
      "  1/239 [..............................] - ETA: 11:37:22 - loss: 1.1999 - accuracy: 0.1667"
     ]
    }
   ],
   "source": [
    "learner.lr_find(show_plot=True, max_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 8e-05...\n",
      "Train for 240 steps, validate for 23 steps\n",
      "Epoch 1/4\n",
      "  2/240 [..............................] - ETA: 10:01:53 - loss: 1.2395 - accuracy: 0.3333    "
     ]
    }
   ],
   "source": [
    "learner.fit_onecycle(8e-5, 4)\n",
    "learner.validate(class_names=t.get_classes())\n",
    "learner.view_top_losses(n=25, preproc=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc=t)\n",
    "predictor.predict('Trump is our savior')\n",
    "predictor.explain('Trump is our savior')\n",
    "\n",
    "predictor.save('/rumor_predictor')\n",
    "reloaded_predictor = ktrain.load_predictor('/tmp/my_20newsgroup_predictor')\n",
    "reloaded_predictor.predict('Trump is our savior')\n",
    "reloaded_predictor.predict_proba('Trump is our savior')\n",
    "reloaded_predictor.get_classes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
